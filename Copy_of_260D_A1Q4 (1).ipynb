{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um2GFQ1TXVuh"
      },
      "source": [
        "# \"Mini-batch Gradient Descent with PySpark\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WUtL4y_nXVuo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "#matplotlib.rcParams['text.usetex'] = True\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import r2_score\n",
        "import time\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-qnDZy33XVux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ee47e22-8afe-44f4-cea9-8456a0e3aeed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=94ca74521d2d255f463205176699a9e2f040e4760c67d624b071787a8bb06d62\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ],
      "source": [
        "# Setting up PySpark\n",
        "!pip install pyspark\n",
        "import pyspark\n",
        "sc = pyspark.SparkContext().getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the gradient of MSE loss for every example below."
      ],
      "metadata": {
        "id": "IlDw3eM92xri"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "96BtJGJJXVuy"
      },
      "outputs": [],
      "source": [
        "# define the gradient of MSE for every example\n",
        "def per_example_mse_gradient(example, w):\n",
        "    x = example[:-1]\n",
        "    y = example[-1]\n",
        "    # ... calculate the gradient here\n",
        "    weights_gradient = 2 * (np.dot(w, x) - y) * np.array(x)\n",
        "    return weights_gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4SScBSsUXVuz"
      },
      "outputs": [],
      "source": [
        "# calculate cumulative sum of gradients\n",
        "def cum_sum_gradients(row, next_row):\n",
        "    return [gradient+next_gradient for gradient, next_gradient in zip(row, next_row)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the map and reduce function below, using the per_example_mse_gradient and cum_sum_gradients functions"
      ],
      "metadata": {
        "id": "FeB19MD423ya"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V-4AnXx_XVuz"
      },
      "outputs": [],
      "source": [
        "def distributed_minibatch_gradient_descent(rdd, learning_rate=0.1, n_iters=100, mini_frac=0.1):\n",
        "    w = np.zeros(len(rdd.first()) - 1).tolist()  # -1 because the last value is y\n",
        "    print(rdd.count())\n",
        "    for i in range(n_iters):\n",
        "        mini_batch = rdd.sample(False,mini_frac)\n",
        "        m = mini_batch.count()\n",
        "\n",
        "        # fill out the map and reduce functions using the per_example and cum_sum functions defined above\n",
        "        rdd_gradient = mini_batch.map(lambda example: per_example_mse_gradient(example, w))\\\n",
        "                                 .reduce(cum_sum_gradients)\n",
        "\n",
        "        # scaling with m and learning rate\n",
        "        w_gradient = [learning_rate * (w / m) for w in rdd_gradient]\n",
        "\n",
        "        # updating weights\n",
        "        w = [w_j - w_grad_j for w_j, w_grad_j in zip(w, w_gradient)]\n",
        "\n",
        "        #print([i, m])\n",
        "\n",
        "    return w"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a distributed function for gradient descent (without mini-batching) below"
      ],
      "metadata": {
        "id": "8m_6vKp93B1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write a distributed function for gradient descent (without mini-batching)\n",
        "def distributed_gradient_descent(rdd, learning_rate=0.1, epoch=10):\n",
        "    w = np.zeros(len(rdd.first())-1).tolist() # -1 because the last value is y\n",
        "    m = rdd.count()\n",
        "    for i in range(epoch):\n",
        "        rdd_gradient = rdd.map(lambda example: per_example_mse_gradient(example, w))\\\n",
        "                          .reduce(cum_sum_gradients)\n",
        "\n",
        "        # scaling with m and learning rate\n",
        "        w_gradient = [learning_rate*(w/m) for w in rdd_gradient]\n",
        "\n",
        "        # updating weights\n",
        "        w = [w_j - w_grad_j for w_j, w_grad_j in zip(w, w_gradient)]\n",
        "\n",
        "    return w"
      ],
      "metadata": {
        "id": "VxCY1nAYzUEM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def comparison_with_coef(w, coef):\n",
        "  print(f\"\"\"Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
        "    distributed gradient descent -> {w}\n",
        "    Scikit-Learn's coefficients  -> {coef}\"\"\")"
      ],
      "metadata": {
        "id": "fEbjV_ebbZhO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_kwargs = {\"n_samples\":1_000_00, \"n_features\":12, \"noise\":3, \"coef\":True, \"random_state\":101}"
      ],
      "metadata": {
        "id": "n2Q9tv7OsS4B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_mb_gd(n_iters=100, mini_frac=0.1, learning_rate=0.1):\n",
        "  # Let's create the data\n",
        "  X, y, coef = make_regression(**data_kwargs)\n",
        "  coef = [round(c, 2) for c in coef]\n",
        "\n",
        "  # Parallelize the data to run distributed\n",
        "  rdd = sc.parallelize(np.hstack([X, y.reshape(-1,1)]).tolist()).cache()\n",
        "  print(f\"Our RDD has {rdd.getNumPartitions()} partitions.\")\n",
        "\n",
        "  w = distributed_minibatch_gradient_descent(rdd, n_iters=n_iters, mini_frac=mini_frac, learning_rate=learning_rate)\n",
        "\n",
        "  w = [round(w_j, 2) for w_j in w]\n",
        "  comparison_with_coef(w, coef)"
      ],
      "metadata": {
        "id": "okkLkPBnh38A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_gd(epoch = 10):\n",
        "  # Let's create the data\n",
        "  X, y, coef = make_regression(**data_kwargs)\n",
        "  coef = [round(c, 2) for c in coef]\n",
        "\n",
        "  # Parallelize the data to run distributed\n",
        "  rdd = sc.parallelize(np.hstack([X, y.reshape(-1,1)]).tolist()).cache()\n",
        "  print(f\"Our RDD has {rdd.getNumPartitions()} partitions.\")\n",
        "\n",
        "  w = distributed_gradient_descent(rdd, epoch=epoch)\n",
        "\n",
        "  w = [round(w_j, 2) for w_j in w]\n",
        "  comparison_with_coef(w, coef)"
      ],
      "metadata": {
        "id": "X7TBW-dtjP-g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch=10\n",
        "m_frac=0.4\n",
        "n_iter = int(1.0/m_frac*epoch)\n",
        "print(n_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxs3oaY_Z4H5",
        "outputId": "6080f0de-262e-4bda-a2c4-9ce13e022a1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ShiZQTFnXVuz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e21872-ffb4-4e3f-8cfc-59cd1c2c6d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "100000\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [9.34, 9.13, 63.71, 64.1, 74.71, 85.57, 55.23, -0.01, 54.22, 19.58, 62.32, -0.02]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ],
      "source": [
        "test_mb_gd()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_gd()"
      ],
      "metadata": {
        "id": "7eIAEUpH0e5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9293258a-ed0a-4e35-f540-c301cc9c027d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [8.16, 8.19, 57.14, 57.26, 66.6, 76.73, 49.38, -0.08, 48.35, 17.45, 55.63, 0.26]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Compare your solution for distributed GD and distributed mini-batch DG with\n",
        "that of Sklearn. What do you observe and why?\n",
        "(Select all that are correct):\n",
        "A. Distributed mini-batch GD is converging faster (less time)  \n",
        "B. Distributed GD is converging faster  (less time)  \n",
        "C. Distributed mini-batch GD has a closer solution to that of sklearn  \n",
        "D. Distributed GD has a closer solution to that of sklearn  \n",
        "E. Distributed GD and distributed mini-batch GD behave the same.  "
      ],
      "metadata": {
        "id": "clcEqJS83Ix7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_gd(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7fWSrPGg948",
        "outputId": "dcb89ced-8249-4e5a-bf12-866721c1b6a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [9.19, 9.03, 63.05, 63.38, 73.83, 84.65, 54.61, -0.02, 53.6, 19.35, 61.59, 0.04]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_gd(50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfZaudbfhEnL",
        "outputId": "d63c7278-8b54-41eb-933e-a890f71caba0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [9.34, 9.12, 63.73, 64.11, 74.72, 85.56, 55.23, -0.0, 54.24, 19.59, 62.31, -0.02]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_gd(80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBPRPs2FhJXC",
        "outputId": "3fbc8d2d-28cb-4b39-af46-6eed357644de"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [9.34, 9.12, 63.73, 64.11, 74.72, 85.56, 55.23, -0.0, 54.24, 19.59, 62.31, -0.02]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_gd(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NybOUH0HhKqf",
        "outputId": "7350607c-4912-4e24-ea08-40be95e9b338"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [9.34, 9.12, 63.73, 64.11, 74.72, 85.56, 55.23, -0.0, 54.24, 19.59, 62.31, -0.02]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Increase the number of iterations for distributed GD and compare the solutions. What do you observe?  \n",
        "A. Distributed GD’s solution gets farther away from the sklearn solution  \n",
        "B. Distributed GD’s solution gets closer to the sklearn solution  \n",
        "C. More iterations do not change the output of Distributed GD.  \n"
      ],
      "metadata": {
        "id": "-T9VY8oDZDm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_mb_gd(mini_frac=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp0jkyGMhfZ9",
        "outputId": "225f3836-4255-4574-c412-6152a116fcfa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "100000\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [9.34, 9.12, 63.74, 64.1, 74.71, 85.57, 55.22, -0.0, 54.23, 19.59, 62.31, -0.01]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_mb_gd(mini_frac=0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J8Sl1pthVY6",
        "outputId": "ac6cbc41-52a0-45aa-fd72-d2b6b960c0a8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "100000\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [9.34, 9.12, 63.73, 64.11, 74.72, 85.56, 55.23, -0.0, 54.24, 19.59, 62.31, -0.02]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_mb_gd(mini_frac=0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV69AEt6hhYT",
        "outputId": "b154df75-f3f9-4a6a-9abe-01fdd97296c9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "100000\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [9.34, 9.12, 63.73, 64.11, 74.72, 85.56, 55.23, -0.0, 54.23, 19.59, 62.31, -0.02]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_mb_gd(mini_frac=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efFfRaa9hcYc",
        "outputId": "02d44804-0e93-4279-8be9-2d5afe78cc93"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "100000\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [9.34, 9.12, 63.73, 64.11, 74.72, 85.56, 55.23, -0.0, 54.23, 19.59, 62.31, -0.01]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Change the mini-batch size to a larger batch size for distributed mini-batch GD and compare the solutions again. What do you observe?  \n",
        "A. Distributed mini-batch GD now converges faster (less time)  \n",
        "B. Distributed mini-batch GD now converges slower (less time)  \n",
        "C. Change of batch size does not influence the behavior of Distributed mini-batch GD.    "
      ],
      "metadata": {
        "id": "MlxEi-csZFw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_mb_gd(learning_rate=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiZ9Dh5YhnvU",
        "outputId": "5b345c9d-68f3-4b1f-e192-46dd81470880"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "100000\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [9.34, 9.11, 63.74, 64.1, 74.72, 85.57, 55.21, -0.01, 54.25, 19.57, 62.32, -0.01]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_mb_gd(learning_rate=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYcusIG7h01d",
        "outputId": "933aea05-66f2-4973-8307-0dcb6fdfb030"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "100000\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [7.97, 7.98, 55.5, 55.64, 64.66, 74.57, 47.89, -0.04, 47.01, 16.96, 53.94, 0.29]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_mb_gd(learning_rate=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LITzHWUmh191",
        "outputId": "a5756042-d094-418a-ad8c-74e94a03a11c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "100000\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [1.54, 1.74, 11.69, 11.66, 13.5, 15.72, 10.04, -0.08, 9.84, 3.52, 11.3, 0.19]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_mb_gd(learning_rate=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ5kPQS_h36K",
        "outputId": "a4e971de-c155-49f3-c689-1bdbda5a8c85"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "100000\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [9.32, 9.14, 63.68, 64.09, 74.74, 85.61, 55.22, 0.02, 54.23, 19.6, 62.3, 0.02]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_mb_gd(learning_rate=1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz-EgBOYixW-",
        "outputId": "bda38e37-ddd2-4d8c-87be-c622f0497903"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our RDD has 2 partitions.\n",
            "100000\n",
            "Here is a side-by-side comparison of our coefficients with Scikit-Learn's:\n",
            "    distributed gradient descent -> [150.47, -28.52, -371.69, -611.63, -10.65, -1044.47, -393.71, 403.81, -15.58, 8.11, 102.37, -626.36]\n",
            "    Scikit-Learn's coefficients  -> [9.34, 9.12, 63.74, 64.1, 74.72, 85.55, 55.22, 0.0, 54.25, 19.59, 62.32, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Select the learning rate for distributed mini-batch GD with a batch size of 40\\% (on the master node), that converges closest to the sklearn solution in 10 iterations.  \n",
        "A. 0.1  \n",
        "B. 0.01  \n",
        "C. 0.001   \n",
        "D. 0.5  \n",
        "E. 1.0"
      ],
      "metadata": {
        "id": "Eod0oR-hZHjV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "lHIxnD2hXVu1"
      },
      "outputs": [],
      "source": [
        "# shutting down spark context\n",
        "sc.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}